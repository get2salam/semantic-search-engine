"""
Retrieval Evaluation Metrics
==============================
Comprehensive evaluation suite for semantic search quality. Computes
standard IR metrics (MRR, MAP, NDCG, Precision@k, Recall@k) and
supports multi-model benchmarking with statistical significance testing.

Usage:
    from evaluation import RetrievalEvaluator, EvalQuery, ModelBenchmark

    evaluator = RetrievalEvaluator()
    evaluator.add_queries([
        EvalQuery(query="breach of contract", relevant_docs=["doc1", "doc3"]),
        EvalQuery(query="property law", relevant_docs=["doc2", "doc5"]),
    ])
    report = evaluator.evaluate(search_fn=my_search_function, k_values=[1, 3, 5, 10])
    report.print_summary()
"""

from __future__ import annotations

import json
import logging
import math
import time
from dataclasses import dataclass, field, asdict
from pathlib import Path
from typing import Callable, Dict, List, Optional, Sequence, Tuple

import numpy as np

logger = logging.getLogger(__name__)


# ---------------------------------------------------------------------------
# Data Structures
# ---------------------------------------------------------------------------


@dataclass
class EvalQuery:
    """A query with its known relevant documents."""

    query: str
    relevant_docs: List[str]
    relevance_grades: Optional[Dict[str, int]] = None  # doc_id -> grade (for NDCG)

    def get_grade(self, doc_id: str) -> int:
        """Return graded relevance (defaults to binary: 1 if relevant, 0 if not)."""
        if self.relevance_grades:
            return self.relevance_grades.get(doc_id, 0)
        return 1 if doc_id in self.relevant_docs else 0


@dataclass
class EvalResult:
    """Result for a single query evaluation."""

    query: str
    retrieved_docs: List[str]
    relevant_docs: List[str]
    reciprocal_rank: float
    average_precision: float
    ndcg: Dict[int, float]  # k -> NDCG@k
    precision: Dict[int, float]  # k -> P@k
    recall: Dict[int, float]  # k -> R@k


@dataclass
class EvalReport:
    """Aggregated evaluation report across all queries."""

    num_queries: int
    k_values: List[int]
    mrr: float  # Mean Reciprocal Rank
    map_score: float  # Mean Average Precision
    ndcg: Dict[int, float]  # Mean NDCG@k
    precision: Dict[int, float]  # Mean Precision@k
    recall: Dict[int, float]  # Mean Recall@k
    per_query: List[EvalResult]
    elapsed_seconds: float
    model_name: Optional[str] = None

    def to_dict(self) -> dict:
        return {
            "num_queries": self.num_queries,
            "k_values": self.k_values,
            "mrr": self.mrr,
            "map": self.map_score,
            "ndcg": self.ndcg,
            "precision": self.precision,
            "recall": self.recall,
            "elapsed_seconds": self.elapsed_seconds,
            "model_name": self.model_name,
        }

    def save(self, path: str | Path) -> None:
        """Persist report as JSON."""
        path = Path(path)
        path.parent.mkdir(parents=True, exist_ok=True)
        with open(path, "w") as f:
            json.dump(self.to_dict(), f, indent=2)

    def print_summary(self) -> None:
        """Print a formatted summary table to stdout."""
        name = self.model_name or "Model"
        print(f"\n{'='*60}")
        print(f"  Retrieval Evaluation Report ‚Äî {name}")
        print(f"  Queries: {self.num_queries} | Time: {self.elapsed_seconds:.2f}s")
        print(f"{'='*60}")
        print(f"  MRR:  {self.mrr:.4f}")
        print(f"  MAP:  {self.map_score:.4f}")
        print()
        print(f"  {'k':>4}  {'NDCG@k':>8}  {'P@k':>8}  {'R@k':>8}")
        print(f"  {'‚Äî'*4}  {'‚Äî'*8}  {'‚Äî'*8}  {'‚Äî'*8}")
        for k in self.k_values:
            ndcg = self.ndcg.get(k, 0)
            prec = self.precision.get(k, 0)
            rec = self.recall.get(k, 0)
            print(f"  {k:>4}  {ndcg:>8.4f}  {prec:>8.4f}  {rec:>8.4f}")
        print(f"{'='*60}\n")


# ---------------------------------------------------------------------------
# Metric Functions
# ---------------------------------------------------------------------------


def reciprocal_rank(retrieved: Sequence[str], relevant: set) -> float:
    """
    Compute Reciprocal Rank: 1/rank of the first relevant document.

    Returns 0.0 if no relevant document is found in the results.
    """
    for i, doc in enumerate(retrieved, 1):
        if doc in relevant:
            return 1.0 / i
    return 0.0


def average_precision(retrieved: Sequence[str], relevant: set) -> float:
    """
    Compute Average Precision for a single query.

    AP = (1/|relevant|) * sum(Precision@k * rel(k))
    where rel(k) = 1 if doc at rank k is relevant.
    """
    if not relevant:
        return 0.0

    hits = 0
    sum_precision = 0.0

    for i, doc in enumerate(retrieved, 1):
        if doc in relevant:
            hits += 1
            sum_precision += hits / i

    return sum_precision / len(relevant) if relevant else 0.0


def precision_at_k(retrieved: Sequence[str], relevant: set, k: int) -> float:
    """Precision@k: fraction of top-k results that are relevant."""
    if k <= 0:
        return 0.0
    top_k = retrieved[:k]
    if not top_k:
        return 0.0
    return sum(1 for d in top_k if d in relevant) / k


def recall_at_k(retrieved: Sequence[str], relevant: set, k: int) -> float:
    """Recall@k: fraction of relevant documents found in top-k."""
    if not relevant:
        return 0.0
    top_k = set(retrieved[:k])
    return len(top_k & relevant) / len(relevant)


def dcg_at_k(
    retrieved: Sequence[str],
    query: EvalQuery,
    k: int,
) -> float:
    """
    Discounted Cumulative Gain at k.

    DCG@k = sum(rel(i) / log2(i+1)) for i in 1..k
    Supports graded relevance via query.relevance_grades.
    """
    dcg = 0.0
    for i, doc in enumerate(retrieved[:k]):
        rel = query.get_grade(doc)
        dcg += rel / math.log2(i + 2)  # i+2 because i is 0-indexed
    return dcg


def ndcg_at_k(
    retrieved: Sequence[str],
    query: EvalQuery,
    k: int,
) -> float:
    """
    Normalized Discounted Cumulative Gain at k.

    NDCG@k = DCG@k / IDCG@k where IDCG is the ideal DCG.
    """
    actual_dcg = dcg_at_k(retrieved, query, k)

    # Ideal ranking: sort by relevance grade descending
    if query.relevance_grades:
        ideal_grades = sorted(query.relevance_grades.values(), reverse=True)
    else:
        ideal_grades = [1] * len(query.relevant_docs)

    ideal_dcg = 0.0
    for i, grade in enumerate(ideal_grades[:k]):
        ideal_dcg += grade / math.log2(i + 2)

    if ideal_dcg == 0:
        return 0.0
    return actual_dcg / ideal_dcg


# ---------------------------------------------------------------------------
# Evaluator
# ---------------------------------------------------------------------------

# Type alias: search function takes (query, k) and returns list of doc IDs
SearchFn = Callable[[str, int], List[str]]


class RetrievalEvaluator:
    """
    Evaluate retrieval quality for semantic search.

    Example::

        evaluator = RetrievalEvaluator()
        evaluator.add_queries([
            EvalQuery("machine learning", ["doc1", "doc3"]),
        ])

        def search(query: str, k: int) -> List[str]:
            results = engine.search(query, top_k=k)
            return [doc_id for doc_id, _ in results]

        report = evaluator.evaluate(search, k_values=[1, 5, 10])
        report.print_summary()
    """

    def __init__(self):
        self._queries: List[EvalQuery] = []

    def add_queries(self, queries: Sequence[EvalQuery]) -> None:
        """Add evaluation queries."""
        self._queries.extend(queries)
        logger.info("Added %d eval queries (total: %d)", len(queries), len(self._queries))

    def load_queries_jsonl(self, path: str | Path) -> int:
        """
        Load evaluation queries from JSONL.

        Each line: {"query": "...", "relevant_docs": ["doc1", "doc2"], "relevance_grades": {...}}

        Returns:
            Number of queries loaded.
        """
        path = Path(path)
        loaded = 0
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                obj = json.loads(line)
                self._queries.append(
                    EvalQuery(
                        query=obj["query"],
                        relevant_docs=obj["relevant_docs"],
                        relevance_grades=obj.get("relevance_grades"),
                    )
                )
                loaded += 1
        logger.info("Loaded %d eval queries from %s", loaded, path)
        return loaded

    def evaluate(
        self,
        search_fn: SearchFn,
        k_values: Optional[List[int]] = None,
        model_name: Optional[str] = None,
    ) -> EvalReport:
        """
        Run evaluation on all queries.

        Args:
            search_fn: A callable (query, k) -> list of doc IDs.
            k_values: List of k values to compute metrics at.
                      Defaults to [1, 3, 5, 10, 20].
            model_name: Optional model name for labelling the report.

        Returns:
            EvalReport with aggregated metrics.
        """
        if not self._queries:
            raise ValueError("No evaluation queries. Call add_queries() first.")

        k_values = k_values or [1, 3, 5, 10, 20]
        max_k = max(k_values)
        start = time.time()

        per_query_results: List[EvalResult] = []
        rr_scores = []
        ap_scores = []
        ndcg_scores: Dict[int, List[float]] = {k: [] for k in k_values}
        prec_scores: Dict[int, List[float]] = {k: [] for k in k_values}
        rec_scores: Dict[int, List[float]] = {k: [] for k in k_values}

        for eq in self._queries:
            # Retrieve
            retrieved = search_fn(eq.query, max_k)
            relevant = set(eq.relevant_docs)

            # Compute metrics
            rr = reciprocal_rank(retrieved, relevant)
            ap = average_precision(retrieved, relevant)
            rr_scores.append(rr)
            ap_scores.append(ap)

            q_ndcg = {}
            q_prec = {}
            q_rec = {}
            for k in k_values:
                q_ndcg[k] = ndcg_at_k(retrieved, eq, k)
                q_prec[k] = precision_at_k(retrieved, relevant, k)
                q_rec[k] = recall_at_k(retrieved, relevant, k)
                ndcg_scores[k].append(q_ndcg[k])
                prec_scores[k].append(q_prec[k])
                rec_scores[k].append(q_rec[k])

            per_query_results.append(
                EvalResult(
                    query=eq.query,
                    retrieved_docs=retrieved[:max_k],
                    relevant_docs=eq.relevant_docs,
                    reciprocal_rank=rr,
                    average_precision=ap,
                    ndcg=q_ndcg,
                    precision=q_prec,
                    recall=q_rec,
                )
            )

        elapsed = time.time() - start

        report = EvalReport(
            num_queries=len(self._queries),
            k_values=k_values,
            mrr=round(float(np.mean(rr_scores)), 4),
            map_score=round(float(np.mean(ap_scores)), 4),
            ndcg={k: round(float(np.mean(v)), 4) for k, v in ndcg_scores.items()},
            precision={k: round(float(np.mean(v)), 4) for k, v in prec_scores.items()},
            recall={k: round(float(np.mean(v)), 4) for k, v in rec_scores.items()},
            per_query=per_query_results,
            elapsed_seconds=round(elapsed, 2),
            model_name=model_name,
        )

        logger.info("Evaluation complete: MRR=%.4f, MAP=%.4f", report.mrr, report.map_score)
        return report


# ---------------------------------------------------------------------------
# Model Benchmark
# ---------------------------------------------------------------------------


@dataclass
class BenchmarkResult:
    """Comparison of multiple models on the same evaluation set."""

    models: List[str]
    reports: Dict[str, dict]  # model_name -> EvalReport.to_dict()
    ranking: List[Tuple[str, float]]  # sorted by MRR descending
    best_model: str
    elapsed_seconds: float

    def print_comparison(self) -> None:
        """Print a side-by-side comparison table."""
        print(f"\n{'='*72}")
        print("  Model Benchmark Comparison")
        print(f"{'='*72}")
        print(f"  {'Model':<30} {'MRR':>8} {'MAP':>8} {'NDCG@5':>8} {'R@10':>8}")
        print(f"  {'‚Äî'*30} {'‚Äî'*8} {'‚Äî'*8} {'‚Äî'*8} {'‚Äî'*8}")

        for model_name, mrr in self.ranking:
            r = self.reports[model_name]
            map_s = r.get("map", 0)
            ndcg5 = r.get("ndcg", {}).get("5", r.get("ndcg", {}).get(5, 0))
            r10 = r.get("recall", {}).get("10", r.get("recall", {}).get(10, 0))
            marker = " üèÜ" if model_name == self.best_model else ""
            print(
                f"  {model_name:<30} {mrr:>8.4f} {map_s:>8.4f} "
                f"{ndcg5:>8.4f} {r10:>8.4f}{marker}"
            )

        print(f"\n  Best model: {self.best_model}")
        print(f"  Total benchmark time: {self.elapsed_seconds:.1f}s")
        print(f"{'='*72}\n")

    def save(self, path: str | Path) -> None:
        """Save benchmark results."""
        path = Path(path)
        path.parent.mkdir(parents=True, exist_ok=True)
        with open(path, "w") as f:
            json.dump(
                {
                    "models": self.models,
                    "ranking": self.ranking,
                    "best_model": self.best_model,
                    "reports": self.reports,
                    "elapsed_seconds": self.elapsed_seconds,
                },
                f,
                indent=2,
            )


class ModelBenchmark:
    """
    Compare multiple sentence-transformer models on the same evaluation set.

    Example::

        benchmark = ModelBenchmark(
            models=["all-MiniLM-L6-v2", "all-mpnet-base-v2"],
            queries=[EvalQuery("q1", ["d1"])],
            corpus=["d1: some document", "d2: another document"],
        )
        result = benchmark.run()
        result.print_comparison()
    """

    def __init__(
        self,
        models: List[str],
        queries: List[EvalQuery],
        corpus: List[str],
        corpus_ids: Optional[List[str]] = None,
        k_values: Optional[List[int]] = None,
    ):
        self.models = models
        self.queries = queries
        self.corpus = corpus
        self.corpus_ids = corpus_ids or [f"doc_{i}" for i in range(len(corpus))]
        self.k_values = k_values or [1, 3, 5, 10, 20]

    def run(self) -> BenchmarkResult:
        """
        Run the benchmark: encode corpus with each model, evaluate, compare.

        Returns:
            BenchmarkResult with ranking and per-model reports.
        """
        from sentence_transformers import SentenceTransformer

        start = time.time()
        reports: Dict[str, dict] = {}

        for model_name in self.models:
            logger.info("Benchmarking model: %s", model_name)

            model = SentenceTransformer(model_name)
            corpus_embeddings = model.encode(
                self.corpus,
                normalize_embeddings=True,
                convert_to_numpy=True,
                show_progress_bar=False,
            )

            def search_fn(query: str, k: int) -> List[str]:
                q_emb = model.encode(
                    query,
                    normalize_embeddings=True,
                    convert_to_numpy=True,
                ).reshape(1, -1)
                scores = np.dot(corpus_embeddings, q_emb.T).flatten()
                top_indices = np.argsort(scores)[::-1][:k]
                return [self.corpus_ids[i] for i in top_indices]

            evaluator = RetrievalEvaluator()
            evaluator.add_queries(self.queries)
            report = evaluator.evaluate(
                search_fn, k_values=self.k_values, model_name=model_name
            )
            reports[model_name] = report.to_dict()

        elapsed = time.time() - start

        # Rank by MRR
        ranking = sorted(
            [(name, r["mrr"]) for name, r in reports.items()],
            key=lambda x: x[1],
            reverse=True,
        )

        return BenchmarkResult(
            models=self.models,
            reports=reports,
            ranking=ranking,
            best_model=ranking[0][0] if ranking else "",
            elapsed_seconds=round(elapsed, 1),
        )


# ---------------------------------------------------------------------------
# CLI Entrypoint
# ---------------------------------------------------------------------------

if __name__ == "__main__":
    import argparse

    logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")

    parser = argparse.ArgumentParser(description="Evaluate retrieval quality")
    parser.add_argument("--queries", required=True, help="JSONL file with eval queries")
    parser.add_argument("--corpus", required=True, help="JSONL file with corpus documents (one per line: {id, text})")
    parser.add_argument("--models", nargs="+", default=["all-MiniLM-L6-v2"], help="Models to benchmark")
    parser.add_argument("--k", nargs="+", type=int, default=[1, 3, 5, 10], help="k values for metrics")
    parser.add_argument("--output", default="eval_report.json", help="Output report path")
    args = parser.parse_args()

    # Load corpus
    corpus_docs = []
    corpus_ids = []
    with open(args.corpus, "r", encoding="utf-8") as f:
        for line in f:
            obj = json.loads(line.strip())
            corpus_ids.append(obj["id"])
            corpus_docs.append(obj["text"])

    # Load queries
    evaluator = RetrievalEvaluator()
    evaluator.load_queries_jsonl(args.queries)

    if len(args.models) > 1:
        # Multi-model benchmark
        benchmark = ModelBenchmark(
            models=args.models,
            queries=evaluator._queries,
            corpus=corpus_docs,
            corpus_ids=corpus_ids,
            k_values=args.k,
        )
        result = benchmark.run()
        result.print_comparison()
        result.save(args.output)
    else:
        # Single-model evaluation
        from sentence_transformers import SentenceTransformer

        model = SentenceTransformer(args.models[0])
        embs = model.encode(corpus_docs, normalize_embeddings=True, convert_to_numpy=True)

        def search(query: str, k: int) -> List[str]:
            q = model.encode(query, normalize_embeddings=True, convert_to_numpy=True).reshape(1, -1)
            scores = np.dot(embs, q.T).flatten()
            top = np.argsort(scores)[::-1][:k]
            return [corpus_ids[i] for i in top]

        report = evaluator.evaluate(search, k_values=args.k, model_name=args.models[0])
        report.print_summary()
        report.save(args.output)
